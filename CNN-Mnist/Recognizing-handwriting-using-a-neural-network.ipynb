{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/emory/.juliapro/JuliaPro_v1.0.5-2/compiled/v1.0/JuliaAcademyData/lrk4V.ji for JuliaAcademyData [18b7da76-0988-5e3b-acac-6290be3a708f]\n",
      "└ @ Base loading.jl:1190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.juliapro/JuliaPro_v1.0.5-2/registries/JuliaPro`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://pkg.juliacomputing.com//registry/JuliaPro`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#import Pkg; Pkg.add(Pkg.PackageSpec(url=\"https://github.com/JuliaComputing/JuliaAcademyData.jl\"))\n",
    "using JuliaAcademyData; activate(\"Deep learning with Flux\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br /><br /><br />\n",
    "# Recognizing handwritten digits using a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now reached the point where we can tackle a very interesting task: applying the knowledge we have gained with machine learning in general, and `Flux.jl` in particular, to create a neural network that can recognize handwritten digits! The data are from a data set called MNIST, which has become a classic in the machine learning world.\n",
    "\n",
    "[We could also try to apply the techniques to the original images of fruit instead. However, the fruit images are much larger than the MNIST images, which makes the learning a suitable neural network too slow.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, the first difficulty with any new data set is locating it, understanding what format it is stored in, reading it in and decoding it into a useful data structure in Julia.\n",
    "\n",
    "The original MNIST data is available [here](http://yann.lecun.com/exdb/mnist); see also the [Wikipedia page](https://en.wikipedia.org/wiki/MNIST_database). However, the format that the data is stored in is rather obscure.\n",
    "\n",
    "Fortunately, various packages in Julia provide nicer interfaces to access it. We will use the one provided by `Flux.jl`.\n",
    "\n",
    "The data are images of handwritten digits, and the corresponding labels that were determined by hand (i.e. by humans). Our job will be to get the computer to **learn** to recognize digits by learning, as usual, the function that relates the input and output data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and examining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: CUDAdrv.jl failed to initialize, GPU functionality unavailable (set JULIA_CUDA_SILENT or JULIA_CUDA_VERBOSE to silence or expand this message)\n",
      "└ @ CUDAdrv /home/emory/.juliapro/JuliaPro_v1.0.5-2/packages/CUDAdrv/aBgcd/src/CUDAdrv.jl:69\n"
     ]
    }
   ],
   "source": [
    "using Flux, Flux.Data.MNIST, Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = MNIST.labels();\n",
    "images = MNIST.images();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `images` is a `Vector`, i.e. an `Array{T, 1}` with a complicated parameter `T`. It has length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can just look at the first handful to get a sense of the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[1:5]' # transposed to match the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the $i$th entry of the array is the data for the $i$th image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(images[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the fruit images from the start of the course, the image is an array of color blocks, except that now each pixel just has a grey scale.\n",
    "\n",
    "To see the actual numeric content of the image, we can do, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Float64.(images[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebooks, we arranged the input data for Flux as a `Vector` of `Vector`s.\n",
    "Now we will use an alternative arrangement, as a matrix, since that allows `Flux` to use matrix operations, which are more efficient.\n",
    "\n",
    "The column $i$ of the matrix is a vector consisting of the $i$th data point $\\mathbf{x}^{(i)}$.  Similarly, the desired outputs are given as a matrix, with the $i$th column being the desired output $\\mathbf{y}^{(i)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = unique(length.(images))[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_outputs = length(unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the features\n",
    "\n",
    "We want to create a vector of feature vectors, each with the floating point values of the 784 pixels.\n",
    "\n",
    "An image is a matrix of colours, but now we need a vector of floating point numbers instead. To do so, we just arrange all of the elements of the matrix in a certain way into a single list; fortunately, Julia already provides the function `vec` to do so!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a subset of $N=5,000$ of the total $60,000$ images available in order to hold out test images that our model hasn't been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocess (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(img) = vec(Float64.(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = preprocess.(images[1:5000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the labels\n",
    "\n",
    "We can just use `Flux.onehot` to create them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [Flux.onehot(label, 0:9) for label in labels[1:5000]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the batched matrices for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a function so we can easily create independent batches from arbitrary segments of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_batch (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function create_batch(r)\n",
    "    xs = [preprocess(img) for img in images[r]]\n",
    "    ys = [Flux.onehot(label, 0:9) for label in labels[r]]\n",
    "    return (Flux.batch(xs), Flux.batch(ys))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train our model on the first 5000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainbatch = create_batch(1:5000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must set up a neural network. Since the data is complicated, we may expect to need several layers.\n",
    "But we can start with a single layer.\n",
    "\n",
    "- The network will take as inputs the vectors $\\mathbf{x}^{(i)}$, so the input layer has $n$ nodes.\n",
    "\n",
    "- The output will be a one-hot vector encoding the digit from 1 to 9 or 0 that is desired. There are 10 possible categories, so we need an output layer of size 10.\n",
    "\n",
    "It is then our task as neural network designers to insert layers between these input and output layers, whose weights will be tuned during the learning process. *This is an art, not a science*! But major advances have come from finding a good structure for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.016015 seconds (19.75 M allocations: 379.569 MiB, 32.74% gc time)\n",
      "  0.824482 seconds (19.75 M allocations: 379.569 MiB, 21.22% gc time)\n"
     ]
    }
   ],
   "source": [
    "model = Chain(Dense(n_inputs, n_outputs, identity), softmax)\n",
    "L(x,y) = Flux.crossentropy(model(x), y)\n",
    "opt = ADAM()\n",
    "@time Flux.train!(L, params(model), [trainbatch], opt)\n",
    "@time Flux.train!(L, params(model), [trainbatch], opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as before, use `repeated` to create an **iterator**. It does not copy the data 100 times, which would be very wasteful; it just gives an object that repeatedly loops over the same data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iterators.repeated(trainbatch, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what the total current loss is (after training just a handful to times above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.288537f0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(trainbatch...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.813549 seconds (19.75 M allocations: 379.569 MiB, 24.84% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time Flux.train!(L, params(model), [trainbatch], opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.237859f0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(trainbatch...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train!` function can take an optional keyword argument, `cb` (short for \"**c**all**b**ack\"). A callback function is a function that you provide as an argument to a function `f`, which \"calls back\" your function every so often.\n",
    "\n",
    "This provides the possibility to provide a function that is called at each step or every so often during the training process.\n",
    "A common use case is to provide a visual trace of the training process by printing out the current value of the `loss` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L(trainbatch...) = 2.1887546f0\n",
      "L(trainbatch...) = 2.1410909f0\n",
      "L(trainbatch...) = 2.0947537f0\n"
     ]
    }
   ],
   "source": [
    "callback() = @show(L(trainbatch...))\n",
    "\n",
    "Flux.train!(L, params(model), Iterators.repeated(trainbatch, 3), opt; cb = callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is expensive to calculate the complete `loss` function and it is not necessary to output it every step. So `Flux` also provides a function `throttle`, that provides a mechanism to call a given function at most once every certain number of seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L(trainbatch...) = 0.93737906f0\n",
      "L(trainbatch...) = 0.8939925f0\n",
      "L(trainbatch...) = 0.85545f0\n",
      "L(trainbatch...) = 0.8210369f0\n",
      "L(trainbatch...) = 0.79015064f0\n",
      "L(trainbatch...) = 0.7622841f0\n",
      "L(trainbatch...) = 0.737014f0\n",
      "L(trainbatch...) = 0.7139883f0\n",
      "L(trainbatch...) = 0.69291353f0\n",
      "L(trainbatch...) = 0.6735435f0\n"
     ]
    }
   ],
   "source": [
    "Flux.train!(L, params(model), Iterators.repeated(trainbatch, 40), opt; cb = Flux.throttle(callback, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, that's just measuring the loss over the same data it's training on. It'd be more representative to test against novel data. In fact, let's track the performance of both as we continue training our model. In order to do so, we need to create a batch of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbatch = create_batch(5001:10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_loss! (generic function with 1 method)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Printf\n",
    "train_loss = Float64[]\n",
    "test_loss = Float64[]\n",
    "function update_loss!()\n",
    "    push!(train_loss, L(trainbatch...))\n",
    "    push!(test_loss, L(testbatch...))\n",
    "    @printf(\"train loss = %.2f, test loss = %.2f\\n\", train_loss[end], test_loss[end])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 0.66, test loss = 0.70\n",
      "train loss = 0.64, test loss = 0.68\n",
      "train loss = 0.62, test loss = 0.67\n",
      "train loss = 0.61, test loss = 0.65\n",
      "train loss = 0.60, test loss = 0.64\n",
      "train loss = 0.58, test loss = 0.63\n",
      "train loss = 0.57, test loss = 0.62\n",
      "train loss = 0.56, test loss = 0.61\n",
      "train loss = 0.55, test loss = 0.60\n",
      "train loss = 0.54, test loss = 0.59\n",
      "train loss = 0.53, test loss = 0.58\n",
      "train loss = 0.52, test loss = 0.57\n",
      "train loss = 0.51, test loss = 0.57\n",
      "train loss = 0.51, test loss = 0.56\n",
      "train loss = 0.50, test loss = 0.55\n",
      "train loss = 0.49, test loss = 0.55\n",
      "train loss = 0.48, test loss = 0.54\n",
      "train loss = 0.48, test loss = 0.53\n",
      "train loss = 0.47, test loss = 0.53\n",
      "train loss = 0.46, test loss = 0.52\n",
      "train loss = 0.46, test loss = 0.52\n",
      "train loss = 0.45, test loss = 0.51\n",
      "train loss = 0.45, test loss = 0.51\n",
      "train loss = 0.44, test loss = 0.50\n",
      "train loss = 0.44, test loss = 0.50\n",
      "train loss = 0.43, test loss = 0.49\n",
      "train loss = 0.43, test loss = 0.49\n",
      "train loss = 0.42, test loss = 0.49\n",
      "train loss = 0.42, test loss = 0.48\n",
      "train loss = 0.41, test loss = 0.48\n",
      "train loss = 0.41, test loss = 0.48\n",
      "train loss = 0.41, test loss = 0.47\n",
      "train loss = 0.40, test loss = 0.47\n",
      "train loss = 0.40, test loss = 0.47\n",
      "train loss = 0.39, test loss = 0.46\n",
      "train loss = 0.39, test loss = 0.46\n",
      "train loss = 0.39, test loss = 0.46\n",
      "train loss = 0.38, test loss = 0.46\n",
      "train loss = 0.38, test loss = 0.45\n",
      "train loss = 0.38, test loss = 0.45\n",
      "train loss = 0.37, test loss = 0.45\n",
      "train loss = 0.37, test loss = 0.45\n",
      "train loss = 0.37, test loss = 0.44\n",
      "train loss = 0.36, test loss = 0.44\n",
      "train loss = 0.36, test loss = 0.44\n",
      "train loss = 0.36, test loss = 0.44\n",
      "train loss = 0.36, test loss = 0.44\n",
      "train loss = 0.35, test loss = 0.43\n",
      "train loss = 0.35, test loss = 0.43\n",
      "train loss = 0.35, test loss = 0.43\n",
      "train loss = 0.35, test loss = 0.43\n",
      "train loss = 0.34, test loss = 0.43\n",
      "train loss = 0.34, test loss = 0.42\n",
      "train loss = 0.34, test loss = 0.42\n",
      "train loss = 0.34, test loss = 0.42\n",
      "train loss = 0.33, test loss = 0.42\n",
      "train loss = 0.33, test loss = 0.42\n",
      "train loss = 0.33, test loss = 0.42\n",
      "train loss = 0.33, test loss = 0.42\n",
      "train loss = 0.33, test loss = 0.41\n",
      "train loss = 0.32, test loss = 0.41\n",
      "train loss = 0.32, test loss = 0.41\n",
      "train loss = 0.32, test loss = 0.41\n",
      "train loss = 0.32, test loss = 0.41\n",
      "train loss = 0.32, test loss = 0.41\n",
      "train loss = 0.31, test loss = 0.41\n",
      "train loss = 0.31, test loss = 0.40\n",
      "train loss = 0.31, test loss = 0.40\n",
      "train loss = 0.31, test loss = 0.40\n",
      "train loss = 0.31, test loss = 0.40\n",
      "train loss = 0.30, test loss = 0.40\n",
      "train loss = 0.30, test loss = 0.40\n",
      "train loss = 0.30, test loss = 0.40\n",
      "train loss = 0.30, test loss = 0.40\n",
      "train loss = 0.30, test loss = 0.40\n",
      "train loss = 0.30, test loss = 0.40\n",
      "train loss = 0.29, test loss = 0.39\n",
      "train loss = 0.29, test loss = 0.39\n",
      "train loss = 0.29, test loss = 0.39\n",
      "train loss = 0.29, test loss = 0.39\n",
      "train loss = 0.29, test loss = 0.39\n",
      "train loss = 0.29, test loss = 0.39\n",
      "train loss = 0.29, test loss = 0.39\n",
      "train loss = 0.28, test loss = 0.39\n",
      "train loss = 0.28, test loss = 0.39\n",
      "train loss = 0.28, test loss = 0.39\n",
      "train loss = 0.28, test loss = 0.39\n",
      "train loss = 0.28, test loss = 0.39\n",
      "train loss = 0.28, test loss = 0.38\n",
      "train loss = 0.28, test loss = 0.38\n",
      "train loss = 0.27, test loss = 0.38\n",
      "train loss = 0.27, test loss = 0.38\n",
      "train loss = 0.27, test loss = 0.38\n",
      "train loss = 0.27, test loss = 0.38\n",
      "train loss = 0.27, test loss = 0.38\n",
      "train loss = 0.27, test loss = 0.38\n",
      "train loss = 0.27, test loss = 0.38\n",
      "train loss = 0.27, test loss = 0.38\n",
      "train loss = 0.26, test loss = 0.38\n",
      "train loss = 0.26, test loss = 0.38\n",
      "train loss = 0.26, test loss = 0.38\n",
      "train loss = 0.26, test loss = 0.38\n",
      "train loss = 0.26, test loss = 0.38\n",
      "train loss = 0.26, test loss = 0.38\n",
      "train loss = 0.26, test loss = 0.38\n",
      "train loss = 0.26, test loss = 0.37\n",
      "train loss = 0.26, test loss = 0.37\n",
      "train loss = 0.25, test loss = 0.37\n",
      "train loss = 0.25, test loss = 0.37\n",
      "train loss = 0.25, test loss = 0.37\n",
      "train loss = 0.25, test loss = 0.37\n",
      "train loss = 0.25, test loss = 0.37\n",
      "train loss = 0.25, test loss = 0.37\n",
      "train loss = 0.25, test loss = 0.37\n",
      "train loss = 0.25, test loss = 0.37\n",
      "train loss = 0.25, test loss = 0.37\n",
      "train loss = 0.25, test loss = 0.37\n",
      "train loss = 0.24, test loss = 0.37\n",
      "train loss = 0.24, test loss = 0.37\n",
      "train loss = 0.24, test loss = 0.37\n",
      "train loss = 0.24, test loss = 0.37\n",
      "train loss = 0.24, test loss = 0.37\n",
      "train loss = 0.24, test loss = 0.37\n",
      "train loss = 0.24, test loss = 0.37\n",
      "train loss = 0.24, test loss = 0.37\n",
      "train loss = 0.24, test loss = 0.37\n",
      "train loss = 0.24, test loss = 0.37\n",
      "train loss = 0.24, test loss = 0.37\n",
      "train loss = 0.23, test loss = 0.37\n",
      "train loss = 0.23, test loss = 0.37\n",
      "train loss = 0.23, test loss = 0.37\n",
      "train loss = 0.23, test loss = 0.36\n",
      "train loss = 0.23, test loss = 0.36\n",
      "train loss = 0.23, test loss = 0.36\n",
      "train loss = 0.23, test loss = 0.36\n",
      "train loss = 0.23, test loss = 0.36\n",
      "train loss = 0.23, test loss = 0.36\n",
      "train loss = 0.23, test loss = 0.36\n",
      "train loss = 0.23, test loss = 0.36\n",
      "train loss = 0.23, test loss = 0.36\n",
      "train loss = 0.22, test loss = 0.36\n",
      "train loss = 0.22, test loss = 0.36\n",
      "train loss = 0.22, test loss = 0.36\n",
      "train loss = 0.22, test loss = 0.36\n",
      "train loss = 0.22, test loss = 0.36\n",
      "train loss = 0.22, test loss = 0.36\n",
      "train loss = 0.22, test loss = 0.36\n",
      "train loss = 0.22, test loss = 0.36\n",
      "train loss = 0.22, test loss = 0.36\n",
      "train loss = 0.22, test loss = 0.36\n",
      "train loss = 0.22, test loss = 0.36\n",
      "train loss = 0.22, test loss = 0.36\n",
      "train loss = 0.22, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.21, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.20, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.36\n",
      "train loss = 0.19, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.18, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.35\n",
      "train loss = 0.17, test loss = 0.36\n",
      "train loss = 0.17, test loss = 0.36\n",
      "train loss = 0.17, test loss = 0.36\n",
      "train loss = 0.17, test loss = 0.36\n",
      "train loss = 0.17, test loss = 0.36\n",
      "train loss = 0.17, test loss = 0.36\n",
      "train loss = 0.17, test loss = 0.36\n",
      "train loss = 0.16, test loss = 0.36\n",
      "train loss = 0.16, test loss = 0.36\n",
      "train loss = 0.16, test loss = 0.36\n",
      "train loss = 0.16, test loss = 0.36\n",
      "train loss = 0.16, test loss = 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\n",
      "└ @ Base loading.jl:1192\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip0100\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip0100)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip0101\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip0100)\" d=\"\n",
       "M215.754 1425.62 L2352.76 1425.62 L2352.76 47.2441 L215.754 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip0102\">\n",
       "    <rect x=\"215\" y=\"47\" width=\"2138\" height=\"1379\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  268.139,1425.62 268.139,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  672.966,1425.62 672.966,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1077.79,1425.62 1077.79,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1482.62,1425.62 1482.62,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1887.45,1425.62 1887.45,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2292.27,1425.62 2292.27,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1296.27 2352.76,1296.27 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1052.53 2352.76,1052.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,808.799 2352.76,808.799 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,565.064 2352.76,565.064 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,321.33 2352.76,321.33 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,77.595 2352.76,77.595 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 215.754,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  268.139,1425.62 268.139,1409.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  672.966,1425.62 672.966,1409.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1077.79,1425.62 1077.79,1409.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1482.62,1425.62 1482.62,1409.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1887.45,1425.62 1887.45,1409.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2292.27,1425.62 2292.27,1409.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1296.27 241.398,1296.27 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1052.53 241.398,1052.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,808.799 241.398,808.799 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,565.064 241.398,565.064 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,321.33 241.398,321.33 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,77.595 241.398,77.595 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 268.139, 1479.62)\" x=\"268.139\" y=\"1479.62\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 672.966, 1479.62)\" x=\"672.966\" y=\"1479.62\">50</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1077.79, 1479.62)\" x=\"1077.79\" y=\"1479.62\">100</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1482.62, 1479.62)\" x=\"1482.62\" y=\"1479.62\">150</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1887.45, 1479.62)\" x=\"1887.45\" y=\"1479.62\">200</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2292.27, 1479.62)\" x=\"2292.27\" y=\"1479.62\">250</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1313.77)\" x=\"191.754\" y=\"1313.77\">0.2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1070.03)\" x=\"191.754\" y=\"1070.03\">0.3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 826.299)\" x=\"191.754\" y=\"826.299\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 582.564)\" x=\"191.754\" y=\"582.564\">0.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 338.83)\" x=\"191.754\" y=\"338.83\">0.6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 95.095)\" x=\"191.754\" y=\"95.095\">0.7</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1284.25, 1559.48)\" x=\"1284.25\" y=\"1559.48\">~seconds of training</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 736.431)\" x=\"89.2861\" y=\"736.431\">loss</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  276.235,185.641 284.332,225.984 292.428,263.469 300.525,298.407 308.621,331.065 316.718,361.672 324.815,390.427 332.911,417.506 341.008,443.061 349.104,467.224 \n",
       "  357.201,490.117 365.297,511.842 373.394,532.494 381.49,552.158 389.587,570.907 397.683,588.81 405.78,605.927 413.877,622.314 421.973,638.02 430.07,653.091 \n",
       "  438.166,667.569 446.263,681.491 454.359,694.891 462.456,707.803 470.552,720.254 478.649,732.272 486.745,743.881 494.842,755.104 502.939,765.963 511.035,776.477 \n",
       "  519.132,786.664 527.228,796.542 535.325,806.125 543.421,815.429 551.518,824.467 559.614,833.252 567.711,841.796 575.807,850.11 583.904,858.205 592,866.09 \n",
       "  600.097,873.775 608.194,881.268 616.29,888.578 624.387,895.712 632.483,902.678 640.58,909.482 648.676,916.131 656.773,922.631 664.869,928.988 672.966,935.207 \n",
       "  681.062,941.294 689.159,947.253 697.256,953.089 705.352,958.807 713.449,964.411 721.545,969.905 729.642,975.292 737.738,980.576 745.835,985.762 753.931,990.851 \n",
       "  762.028,995.848 770.124,1000.75 778.221,1005.57 786.318,1010.31 794.414,1014.97 802.511,1019.54 810.607,1024.04 818.704,1028.46 826.8,1032.82 834.897,1037.1 \n",
       "  842.993,1041.32 851.09,1045.46 859.186,1049.55 867.283,1053.57 875.38,1057.54 883.476,1061.44 891.573,1065.29 899.669,1069.08 907.766,1072.81 915.862,1076.5 \n",
       "  923.959,1080.13 932.055,1083.71 940.152,1087.24 948.248,1090.73 956.345,1094.17 964.442,1097.56 972.538,1100.91 980.635,1104.22 988.731,1107.48 996.828,1110.71 \n",
       "  1004.92,1113.89 1013.02,1117.04 1021.12,1120.14 1029.21,1123.21 1037.31,1126.24 1045.41,1129.24 1053.5,1132.2 1061.6,1135.13 1069.7,1138.02 1077.79,1140.88 \n",
       "  1085.89,1143.71 1093.99,1146.51 1102.08,1149.27 1110.18,1152.01 1118.28,1154.71 1126.37,1157.39 1134.47,1160.04 1142.57,1162.66 1150.66,1165.26 1158.76,1167.83 \n",
       "  1166.86,1170.37 1174.95,1172.88 1183.05,1175.37 1191.14,1177.84 1199.24,1180.28 1207.34,1182.7 1215.43,1185.09 1223.53,1187.46 1231.63,1189.81 1239.72,1192.14 \n",
       "  1247.82,1194.44 1255.92,1196.73 1264.01,1198.99 1272.11,1201.23 1280.21,1203.45 1288.3,1205.65 1296.4,1207.84 1304.5,1210 1312.59,1212.14 1320.69,1214.27 \n",
       "  1328.79,1216.37 1336.88,1218.46 1344.98,1220.53 1353.08,1222.58 1361.17,1224.62 1369.27,1226.64 1377.37,1228.64 1385.46,1230.63 1393.56,1232.6 1401.65,1234.55 \n",
       "  1409.75,1236.49 1417.85,1238.41 1425.94,1240.32 1434.04,1242.21 1442.14,1244.09 1450.23,1245.95 1458.33,1247.8 1466.43,1249.63 1474.52,1251.45 1482.62,1253.26 \n",
       "  1490.72,1255.05 1498.81,1256.83 1506.91,1258.6 1515.01,1260.35 1523.1,1262.09 1531.2,1263.82 1539.3,1265.53 1547.39,1267.24 1555.49,1268.93 1563.59,1270.61 \n",
       "  1571.68,1272.27 1579.78,1273.93 1587.88,1275.57 1595.97,1277.2 1604.07,1278.82 1612.17,1280.43 1620.26,1282.03 1628.36,1283.62 1636.45,1285.2 1644.55,1286.77 \n",
       "  1652.65,1288.32 1660.74,1289.87 1668.84,1291.41 1676.94,1292.93 1685.03,1294.45 1693.13,1295.96 1701.23,1297.45 1709.32,1298.94 1717.42,1300.42 1725.52,1301.89 \n",
       "  1733.61,1303.35 1741.71,1304.8 1749.81,1306.24 1757.9,1307.67 1766,1309.09 1774.1,1310.51 1782.19,1311.91 1790.29,1313.31 1798.39,1314.7 1806.48,1316.08 \n",
       "  1814.58,1317.46 1822.68,1318.82 1830.77,1320.18 1838.87,1321.53 1846.96,1322.87 1855.06,1324.2 1863.16,1325.53 1871.25,1326.84 1879.35,1328.15 1887.45,1329.46 \n",
       "  1895.54,1330.75 1903.64,1332.04 1911.74,1333.32 1919.83,1334.6 1927.93,1335.86 1936.03,1337.12 1944.12,1338.38 1952.22,1339.62 1960.32,1340.86 1968.41,1342.1 \n",
       "  1976.51,1343.32 1984.61,1344.54 1992.7,1345.76 2000.8,1346.96 2008.9,1348.16 2016.99,1349.36 2025.09,1350.54 2033.19,1351.73 2041.28,1352.9 2049.38,1354.07 \n",
       "  2057.47,1355.24 2065.57,1356.39 2073.67,1357.54 2081.76,1358.69 2089.86,1359.83 2097.96,1360.97 2106.05,1362.09 2114.15,1363.22 2122.25,1364.33 2130.34,1365.45 \n",
       "  2138.44,1366.55 2146.54,1367.65 2154.63,1368.75 2162.73,1369.84 2170.83,1370.93 2178.92,1372.01 2187.02,1373.08 2195.12,1374.15 2203.21,1375.22 2211.31,1376.28 \n",
       "  2219.41,1377.33 2227.5,1378.38 2235.6,1379.43 2243.7,1380.47 2251.79,1381.5 2259.89,1382.53 2267.99,1383.56 2276.08,1384.58 2284.18,1385.6 2292.27,1386.61 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0102)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  276.235,86.2547 284.332,123.791 292.428,158.56 300.525,190.864 308.621,220.961 316.718,249.075 324.815,275.4 332.911,300.102 341.008,323.329 349.104,345.208 \n",
       "  357.201,365.854 365.297,385.368 373.394,403.841 381.49,421.355 389.587,437.981 397.683,453.785 405.78,468.826 413.877,483.157 421.973,496.826 430.07,509.877 \n",
       "  438.166,522.352 446.263,534.286 454.359,545.715 462.456,556.667 470.552,567.173 478.649,577.259 486.745,586.947 494.842,596.262 502.939,605.223 511.035,613.849 \n",
       "  519.132,622.159 527.228,630.169 535.325,637.894 543.421,645.349 551.518,652.547 559.614,659.501 567.711,666.221 575.807,672.72 583.904,679.007 592,685.093 \n",
       "  600.097,690.986 608.194,696.694 616.29,702.226 624.387,707.59 632.483,712.791 640.58,717.839 648.676,722.737 656.773,727.493 664.869,732.113 672.966,736.601 \n",
       "  681.062,740.962 689.159,745.203 697.256,749.326 705.352,753.338 713.449,757.241 721.545,761.039 729.642,764.737 737.738,768.338 745.835,771.846 753.931,775.263 \n",
       "  762.028,778.593 770.124,781.839 778.221,785.003 786.318,788.089 794.414,791.099 802.511,794.034 810.607,796.899 818.704,799.695 826.8,802.423 834.897,805.087 \n",
       "  842.993,807.688 851.09,810.228 859.186,812.709 867.283,815.133 875.38,817.501 883.476,819.816 891.573,822.077 899.669,824.288 907.766,826.45 915.862,828.563 \n",
       "  923.959,830.629 932.055,832.65 940.152,834.627 948.248,836.56 956.345,838.452 964.442,840.303 972.538,842.114 980.635,843.886 988.731,845.621 996.828,847.318 \n",
       "  1004.92,848.98 1013.02,850.607 1021.12,852.2 1029.21,853.76 1037.31,855.287 1045.41,856.782 1053.5,858.247 1061.6,859.681 1069.7,861.086 1077.79,862.462 \n",
       "  1085.89,863.81 1093.99,865.131 1102.08,866.424 1110.18,867.692 1118.28,868.933 1126.37,870.15 1134.47,871.342 1142.57,872.509 1150.66,873.654 1158.76,874.775 \n",
       "  1166.86,875.874 1174.95,876.95 1183.05,878.005 1191.14,879.039 1199.24,880.052 1207.34,881.045 1215.43,882.018 1223.53,882.971 1231.63,883.905 1239.72,884.821 \n",
       "  1247.82,885.718 1255.92,886.597 1264.01,887.458 1272.11,888.302 1280.21,889.129 1288.3,889.939 1296.4,890.733 1304.5,891.511 1312.59,892.273 1320.69,893.019 \n",
       "  1328.79,893.75 1336.88,894.466 1344.98,895.168 1353.08,895.855 1361.17,896.528 1369.27,897.187 1377.37,897.832 1385.46,898.463 1393.56,899.082 1401.65,899.687 \n",
       "  1409.75,900.28 1417.85,900.86 1425.94,901.428 1434.04,901.983 1442.14,902.527 1450.23,903.059 1458.33,903.579 1466.43,904.087 1474.52,904.585 1482.62,905.071 \n",
       "  1490.72,905.547 1498.81,906.012 1506.91,906.466 1515.01,906.91 1523.1,907.344 1531.2,907.768 1539.3,908.181 1547.39,908.585 1555.49,908.98 1563.59,909.365 \n",
       "  1571.68,909.741 1579.78,910.107 1587.88,910.464 1595.97,910.813 1604.07,911.152 1612.17,911.483 1620.26,911.806 1628.36,912.12 1636.45,912.425 1644.55,912.723 \n",
       "  1652.65,913.012 1660.74,913.294 1668.84,913.567 1676.94,913.833 1685.03,914.091 1693.13,914.342 1701.23,914.585 1709.32,914.821 1717.42,915.05 1725.52,915.272 \n",
       "  1733.61,915.486 1741.71,915.694 1749.81,915.894 1757.9,916.088 1766,916.276 1774.1,916.456 1782.19,916.631 1790.29,916.798 1798.39,916.96 1806.48,917.115 \n",
       "  1814.58,917.264 1822.68,917.407 1830.77,917.544 1838.87,917.675 1846.96,917.8 1855.06,917.919 1863.16,918.033 1871.25,918.141 1879.35,918.244 1887.45,918.34 \n",
       "  1895.54,918.432 1903.64,918.518 1911.74,918.599 1919.83,918.674 1927.93,918.745 1936.03,918.81 1944.12,918.87 1952.22,918.925 1960.32,918.975 1968.41,919.021 \n",
       "  1976.51,919.061 1984.61,919.097 1992.7,919.128 2000.8,919.155 2008.9,919.176 2016.99,919.194 2025.09,919.206 2033.19,919.215 2041.28,919.219 2049.38,919.218 \n",
       "  2057.47,919.214 2065.57,919.205 2073.67,919.192 2081.76,919.174 2089.86,919.153 2097.96,919.128 2106.05,919.098 2114.15,919.065 2122.25,919.027 2130.34,918.986 \n",
       "  2138.44,918.941 2146.54,918.892 2154.63,918.84 2162.73,918.783 2170.83,918.723 2178.92,918.659 2187.02,918.592 2195.12,918.521 2203.21,918.447 2211.31,918.369 \n",
       "  2219.41,918.288 2227.5,918.203 2235.6,918.115 2243.7,918.024 2251.79,917.929 2259.89,917.831 2267.99,917.729 2276.08,917.625 2284.18,917.517 2292.27,917.406 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip0100)\" d=\"\n",
       "M1947.14 312.204 L2280.76 312.204 L2280.76 130.764 L1947.14 130.764  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1947.14,312.204 2280.76,312.204 2280.76,130.764 1947.14,130.764 1947.14,312.204 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1971.14,191.244 2115.14,191.244 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2139.14, 208.744)\" x=\"2139.14\" y=\"208.744\">train</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip0100)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1971.14,251.724 2115.14,251.724 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip0100)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2139.14, 269.224)\" x=\"2139.14\" y=\"269.224\">test</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flux.train!(L, params(model), Iterators.repeated(trainbatch, 1000), opt;\n",
    "                cb = Flux.throttle(update_loss!, 1))\n",
    "using Plots\n",
    "plot(1:length(train_loss), train_loss, xlabel=\"~seconds of training\", ylabel=\"loss\", label=\"train\")\n",
    "plot!(1:length(test_loss), test_loss, label=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have trained a model, i.e. we have found the parameters `W` and `b` for the network layer(s). In order to **test** if the learning procedure was really successful, we check how well the resulting trained network performs when we test it with images that the network has not yet seen!\n",
    "\n",
    "Often, a dataset is split up into \"training data\" and \"test (or validation) data\" for this purpose, and indeed the MNIST data set has a separate pool of training data. We can instead use the images that we have not included in our reduced training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m Installed\u001b[22m\u001b[39m ImageMagick ─ v0.7.5\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.juliapro/JuliaPro_v1.0.5-2/packages/JuliaAcademyData/1to3l/courses/Deep learning with Flux/Project.toml`\n",
      " \u001b[90m [6218d12a]\u001b[39m\u001b[92m + ImageMagick v0.7.5\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.juliapro/JuliaPro_v1.0.5-2/packages/JuliaAcademyData/1to3l/courses/Deep learning with Flux/Manifest.toml`\n",
      " \u001b[90m [6218d12a]\u001b[39m\u001b[92m + ImageMagick v0.7.5\u001b[39m\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m ImageMagick → `~/.juliapro/JuliaPro_v1.0.5-2/packages/ImageMagick/vMfoS/deps/build.log`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.add(\"ImageMagick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling ImageMagick [6218d12a-5da1-5696-b52f-db25d2ecc6d1]\n",
      "└ @ Base loading.jl:1192\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAHmSURBVGje7dm9axRBHMbxj0YFu1QGQTCoUQMWAQut1T9B8AURRES0EQvLdJZGFMHSVrCMVioEVBQLQWJhrUVEfD1PlBQhFjMkF8lebm/BG4d5mpld7n5fnnvutzO7S1FRUVFRUVFRUVFRUXOtq/PhM1jEF4zjBZ7WBK7/1w7zB67I8CQmhKxW03AcF7AJv/ELb3AMn1J0mD9wKcNruIShPgvNCP+Bj6k5zB+4lOF7bMOs0F+deobpVb58GKcxGo9ncFz3fsz/Jx1chruxD4/QrlFgB+4L6yNcwVRKDvMH1trTVOko7sX5Z2xJyWEBFmD6wA1NC1zE/o7jzfH4VSoO8wfWupZuxSlh/9p57u8iPyzfhwzcYf7AnvrwiNBb54Q9zFq6k5LD/IFdMxzDbRyystfe4VucT2Iet7AnnvuQksP8gZUZXhbWup34iRZuYA7PhRw71YpjGw9Scpg/sDLDg0J+07iOJ12KTGB7nM/jbUoO8wdWZnhBeA56tYciuzAS549Tc5g/sDLDr3rLDw7E8buwLiblMH9g4/vDWeyN84fC+6ikHOYPbJzhaCzSws0UHeYPbJThCeG5TBvnrd2DA3GYP7DvZ94b8VK4jt7F2VQd5g/suw8XhexeC++rknWYP7Do/9cfyWhEoqqYtCQAAAAASUVORK5CYII=",
      "text/plain": [
       "28×28 Array{Gray{N0f8},2} with eltype Gray{Normed{UInt8,8}}:\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                                  \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(7, (0.8374565f0, 7))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 5001\n",
    "display(images[i])\n",
    "labels[i], findmax(model(preprocess(images[i]))) .- (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float32,1}:\n",
       " 8.741581e-6  \n",
       " 5.5174223e-6 \n",
       " 0.1494184    \n",
       " 0.010607624  \n",
       " 7.654782e-5  \n",
       " 1.2090718e-6 \n",
       " 3.0641997e-6 \n",
       " 0.8374565    \n",
       " 0.00065429974\n",
       " 0.0017679931 "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(preprocess(images[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "What percent of images are we correctly classifying if we take the highest element to be the chosen answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction (generic function with 1 method)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction(i) = findmax(model(preprocess(images[i])))[2]-1 # returns (max_value, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9658"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(prediction(i) == labels[i] for i in 1:5000)/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8972"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(prediction(i) == labels[i] for i in 5001:10000)/5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have used a single layer. In order to improve the prediction, we probably need to use more layers. Try adding more layers yourself and see how the performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[-0.000252881 0.0795513 … 0.00209301 -0.00939233; -0.0348998 -0.0293833 … 0.0261819 0.0248802; … ; -0.0254793 0.0606648 … 0.0772348 -0.0666949; -0.00117166 0.071862 … -0.0830291 -0.0451185], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.176436 -0.163135 … -0.186514 0.325669; 0.145484 0.101365 … -0.0927699 0.397632; … ; -0.195646 -0.189248 … -0.303186 -0.177641; -0.0555107 0.0333282 … 0.334002 -0.0605369], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_hidden = 20\n",
    "model = Chain(Dense(n_inputs, n_hidden, relu),\n",
    "              Dense(n_hidden, n_outputs, identity), softmax)\n",
    "L(x,y) = Flux.crossentropy(model(x), y)\n",
    "opt = ADAM()\n",
    "par = params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 2.27, test loss = 2.26\n",
      "train loss = 2.12, test loss = 2.12\n",
      "train loss = 1.93, test loss = 1.92\n",
      "train loss = 1.74, test loss = 1.73\n",
      "train loss = 1.57, test loss = 1.57\n",
      "train loss = 1.43, test loss = 1.42\n",
      "train loss = 1.30, test loss = 1.30\n",
      "train loss = 1.19, test loss = 1.19\n",
      "train loss = 1.09, test loss = 1.10\n",
      "train loss = 1.00, test loss = 1.01\n",
      "train loss = 0.92, test loss = 0.94\n",
      "train loss = 0.86, test loss = 0.88\n",
      "train loss = 0.80, test loss = 0.83\n",
      "train loss = 0.74, test loss = 0.78\n",
      "train loss = 0.70, test loss = 0.73\n",
      "train loss = 0.65, test loss = 0.70\n",
      "train loss = 0.62, test loss = 0.66\n",
      "train loss = 0.59, test loss = 0.63\n",
      "train loss = 0.56, test loss = 0.61\n",
      "train loss = 0.53, test loss = 0.59\n",
      "train loss = 0.51, test loss = 0.57\n",
      "train loss = 0.49, test loss = 0.55\n",
      "train loss = 0.47, test loss = 0.53\n",
      "train loss = 0.45, test loss = 0.52\n",
      "train loss = 0.44, test loss = 0.50\n",
      "train loss = 0.42, test loss = 0.49\n",
      "train loss = 0.41, test loss = 0.48\n",
      "train loss = 0.40, test loss = 0.47\n",
      "train loss = 0.39, test loss = 0.46\n",
      "train loss = 0.38, test loss = 0.45\n",
      "train loss = 0.37, test loss = 0.44\n",
      "train loss = 0.36, test loss = 0.44\n",
      "train loss = 0.35, test loss = 0.43\n",
      "train loss = 0.34, test loss = 0.42\n",
      "train loss = 0.33, test loss = 0.42\n",
      "train loss = 0.33, test loss = 0.41\n",
      "train loss = 0.32, test loss = 0.41\n",
      "train loss = 0.31, test loss = 0.40\n",
      "train loss = 0.31, test loss = 0.40\n",
      "train loss = 0.30, test loss = 0.39\n",
      "train loss = 0.30, test loss = 0.39\n",
      "train loss = 0.29, test loss = 0.38\n",
      "train loss = 0.29, test loss = 0.38\n",
      "train loss = 0.28, test loss = 0.38\n",
      "train loss = 0.28, test loss = 0.37\n",
      "train loss = 0.27, test loss = 0.37\n",
      "train loss = 0.27, test loss = 0.37\n",
      "train loss = 0.26, test loss = 0.37\n",
      "train loss = 0.26, test loss = 0.36\n",
      "train loss = 0.25, test loss = 0.36\n",
      "train loss = 0.25, test loss = 0.36\n",
      "train loss = 0.25, test loss = 0.36\n",
      "train loss = 0.24, test loss = 0.35\n",
      "train loss = 0.24, test loss = 0.35\n",
      "train loss = 0.24, test loss = 0.35\n",
      "train loss = 0.23, test loss = 0.35\n",
      "train loss = 0.23, test loss = 0.35\n",
      "train loss = 0.23, test loss = 0.34\n",
      "train loss = 0.22, test loss = 0.34\n",
      "train loss = 0.22, test loss = 0.34\n",
      "train loss = 0.22, test loss = 0.34\n",
      "train loss = 0.22, test loss = 0.34\n",
      "train loss = 0.21, test loss = 0.34\n",
      "train loss = 0.21, test loss = 0.34\n",
      "train loss = 0.21, test loss = 0.34\n",
      "train loss = 0.20, test loss = 0.33\n",
      "train loss = 0.20, test loss = 0.33\n",
      "train loss = 0.20, test loss = 0.33\n",
      "train loss = 0.20, test loss = 0.33\n",
      "train loss = 0.20, test loss = 0.33\n",
      "train loss = 0.19, test loss = 0.33\n",
      "train loss = 0.19, test loss = 0.33\n",
      "train loss = 0.19, test loss = 0.33\n",
      "train loss = 0.19, test loss = 0.33\n",
      "train loss = 0.18, test loss = 0.33\n",
      "train loss = 0.18, test loss = 0.33\n",
      "train loss = 0.18, test loss = 0.33\n",
      "train loss = 0.18, test loss = 0.33\n",
      "train loss = 0.18, test loss = 0.32\n",
      "train loss = 0.17, test loss = 0.32\n",
      "train loss = 0.17, test loss = 0.32\n",
      "train loss = 0.17, test loss = 0.32\n",
      "train loss = 0.17, test loss = 0.32\n",
      "train loss = 0.17, test loss = 0.32\n",
      "train loss = 0.17, test loss = 0.32\n",
      "train loss = 0.16, test loss = 0.32\n",
      "train loss = 0.16, test loss = 0.32\n",
      "train loss = 0.16, test loss = 0.32\n",
      "train loss = 0.16, test loss = 0.32\n",
      "train loss = 0.16, test loss = 0.32\n",
      "train loss = 0.16, test loss = 0.32\n",
      "train loss = 0.15, test loss = 0.32\n",
      "train loss = 0.15, test loss = 0.32\n",
      "train loss = 0.15, test loss = 0.32\n",
      "train loss = 0.15, test loss = 0.32\n",
      "train loss = 0.15, test loss = 0.32\n",
      "train loss = 0.15, test loss = 0.32\n",
      "train loss = 0.14, test loss = 0.32\n",
      "train loss = 0.14, test loss = 0.32\n",
      "train loss = 0.14, test loss = 0.32\n",
      "train loss = 0.14, test loss = 0.32\n",
      "train loss = 0.14, test loss = 0.32\n",
      "train loss = 0.14, test loss = 0.32\n",
      "train loss = 0.14, test loss = 0.32\n",
      "train loss = 0.13, test loss = 0.32\n",
      "train loss = 0.13, test loss = 0.32\n",
      "train loss = 0.13, test loss = 0.32\n",
      "train loss = 0.13, test loss = 0.32\n",
      "train loss = 0.13, test loss = 0.32\n",
      "train loss = 0.13, test loss = 0.32\n",
      "train loss = 0.13, test loss = 0.32\n",
      "train loss = 0.12, test loss = 0.32\n",
      "train loss = 0.12, test loss = 0.32\n",
      "train loss = 0.12, test loss = 0.32\n",
      "train loss = 0.12, test loss = 0.32\n",
      "train loss = 0.12, test loss = 0.32\n",
      "train loss = 0.12, test loss = 0.32\n",
      "train loss = 0.12, test loss = 0.32\n",
      "train loss = 0.12, test loss = 0.32\n",
      "train loss = 0.12, test loss = 0.32\n",
      "train loss = 0.11, test loss = 0.32\n",
      "train loss = 0.11, test loss = 0.32\n",
      "train loss = 0.11, test loss = 0.32\n",
      "train loss = 0.11, test loss = 0.32\n",
      "train loss = 0.11, test loss = 0.32\n",
      "train loss = 0.11, test loss = 0.32\n",
      "train loss = 0.11, test loss = 0.32\n",
      "train loss = 0.11, test loss = 0.32\n",
      "train loss = 0.11, test loss = 0.32\n",
      "train loss = 0.10, test loss = 0.32\n",
      "train loss = 0.10, test loss = 0.32\n",
      "train loss = 0.10, test loss = 0.32\n",
      "train loss = 0.10, test loss = 0.32\n",
      "train loss = 0.10, test loss = 0.32\n",
      "train loss = 0.10, test loss = 0.32\n",
      "train loss = 0.10, test loss = 0.32\n",
      "train loss = 0.10, test loss = 0.32\n",
      "train loss = 0.10, test loss = 0.32\n",
      "train loss = 0.10, test loss = 0.32\n",
      "train loss = 0.09, test loss = 0.32\n",
      "train loss = 0.09, test loss = 0.32\n",
      "train loss = 0.09, test loss = 0.32\n",
      "train loss = 0.09, test loss = 0.32\n",
      "train loss = 0.09, test loss = 0.32\n",
      "train loss = 0.09, test loss = 0.33\n",
      "train loss = 0.09, test loss = 0.33\n",
      "train loss = 0.09, test loss = 0.33\n",
      "train loss = 0.09, test loss = 0.33\n",
      "train loss = 0.09, test loss = 0.33\n",
      "train loss = 0.09, test loss = 0.33\n",
      "train loss = 0.08, test loss = 0.33\n",
      "train loss = 0.08, test loss = 0.33\n",
      "train loss = 0.08, test loss = 0.33\n",
      "train loss = 0.08, test loss = 0.33\n",
      "train loss = 0.08, test loss = 0.33\n",
      "train loss = 0.08, test loss = 0.33\n",
      "train loss = 0.08, test loss = 0.33\n",
      "train loss = 0.08, test loss = 0.33\n",
      "train loss = 0.08, test loss = 0.33\n",
      "train loss = 0.08, test loss = 0.33\n",
      "train loss = 0.08, test loss = 0.33\n",
      "train loss = 0.08, test loss = 0.33\n",
      "train loss = 0.07, test loss = 0.33\n",
      "train loss = 0.07, test loss = 0.33\n",
      "train loss = 0.07, test loss = 0.33\n",
      "train loss = 0.07, test loss = 0.33\n",
      "train loss = 0.07, test loss = 0.33\n",
      "train loss = 0.07, test loss = 0.33\n",
      "train loss = 0.07, test loss = 0.33\n",
      "train loss = 0.07, test loss = 0.33\n",
      "train loss = 0.07, test loss = 0.33\n",
      "train loss = 0.07, test loss = 0.33\n",
      "train loss = 0.07, test loss = 0.34\n",
      "train loss = 0.07, test loss = 0.34\n",
      "train loss = 0.07, test loss = 0.34\n",
      "train loss = 0.07, test loss = 0.34\n",
      "train loss = 0.07, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.06, test loss = 0.34\n",
      "train loss = 0.05, test loss = 0.34\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.05, test loss = 0.35\n",
      "train loss = 0.04, test loss = 0.35\n",
      "train loss = 0.04, test loss = 0.35\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.36\n",
      "train loss = 0.04, test loss = 0.37\n",
      "train loss = 0.04, test loss = 0.37\n",
      "train loss = 0.04, test loss = 0.37\n",
      "train loss = 0.04, test loss = 0.37\n",
      "train loss = 0.04, test loss = 0.37\n",
      "train loss = 0.04, test loss = 0.37\n",
      "train loss = 0.03, test loss = 0.37\n",
      "train loss = 0.03, test loss = 0.37\n",
      "train loss = 0.03, test loss = 0.37\n",
      "train loss = 0.03, test loss = 0.37\n",
      "train loss = 0.03, test loss = 0.37\n",
      "train loss = 0.03, test loss = 0.37\n",
      "train loss = 0.03, test loss = 0.37\n",
      "train loss = 0.03, test loss = 0.37\n",
      "train loss = 0.03, test loss = 0.37\n",
      "train loss = 0.03, test loss = 0.37\n",
      "train loss = 0.03, test loss = 0.37\n",
      "train loss = 0.03, test loss = 0.37\n",
      "train loss = 0.03, test loss = 0.38\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip0500\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip0500)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip0501\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip0500)\" d=\"\n",
       "M215.754 1425.62 L2352.76 1425.62 L2352.76 47.2441 L215.754 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip0502\">\n",
       "    <rect x=\"215\" y=\"47\" width=\"2138\" height=\"1379\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip0502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  268.203,1425.62 268.203,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  669.805,1425.62 669.805,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1071.41,1425.62 1071.41,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1473.01,1425.62 1473.01,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1874.61,1425.62 1874.61,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2276.21,1425.62 2276.21,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1404.67 2352.76,1404.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,1113.92 2352.76,1113.92 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,823.18 2352.76,823.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,532.437 2352.76,532.437 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0502)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  215.754,241.694 2352.76,241.694 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 2352.76,1425.62 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1425.62 215.754,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  268.203,1425.62 268.203,1409.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  669.805,1425.62 669.805,1409.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1071.41,1425.62 1071.41,1409.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1473.01,1425.62 1473.01,1409.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1874.61,1425.62 1874.61,1409.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2276.21,1425.62 2276.21,1409.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1404.67 241.398,1404.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,1113.92 241.398,1113.92 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,823.18 241.398,823.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,532.437 241.398,532.437 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.754,241.694 241.398,241.694 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 268.203, 1479.62)\" x=\"268.203\" y=\"1479.62\">0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 669.805, 1479.62)\" x=\"669.805\" y=\"1479.62\">50</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1071.41, 1479.62)\" x=\"1071.41\" y=\"1479.62\">100</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1473.01, 1479.62)\" x=\"1473.01\" y=\"1479.62\">150</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1874.61, 1479.62)\" x=\"1874.61\" y=\"1479.62\">200</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2276.21, 1479.62)\" x=\"2276.21\" y=\"1479.62\">250</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1422.17)\" x=\"191.754\" y=\"1422.17\">0.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 1131.42)\" x=\"191.754\" y=\"1131.42\">0.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 840.68)\" x=\"191.754\" y=\"840.68\">1.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 549.937)\" x=\"191.754\" y=\"549.937\">1.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 191.754, 259.194)\" x=\"191.754\" y=\"259.194\">2.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1284.25, 1559.48)\" x=\"1284.25\" y=\"1559.48\">~seconds of training</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 89.2861, 736.431)\" x=\"89.2861\" y=\"736.431\">loss</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip0502)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  276.235,86.2547 284.267,170.852 292.299,285.049 300.331,393.498 308.363,489.859 316.395,575.143 324.427,649.446 332.459,714.022 340.492,770.955 348.524,821.654 \n",
       "  356.556,866.801 364.588,906.739 372.62,942.047 380.652,973.114 388.684,1000.37 396.716,1024.31 404.748,1045.35 412.78,1063.95 420.812,1080.45 428.844,1095.18 \n",
       "  436.876,1108.38 444.908,1120.26 452.94,1131 460.972,1140.74 469.004,1149.64 477.036,1157.82 485.068,1165.35 493.1,1172.32 501.132,1178.8 509.164,1184.84 \n",
       "  517.196,1190.5 525.228,1195.8 533.26,1200.79 541.292,1205.5 549.324,1209.95 557.356,1214.16 565.388,1218.16 573.42,1221.96 581.452,1225.57 589.484,1229.02 \n",
       "  597.516,1232.32 605.548,1235.47 613.581,1238.49 621.613,1241.39 629.645,1244.17 637.677,1246.84 645.709,1249.43 653.741,1251.91 661.773,1254.31 669.805,1256.63 \n",
       "  677.837,1258.88 685.869,1261.05 693.901,1263.16 701.933,1265.2 709.965,1267.18 717.997,1269.11 726.029,1270.98 734.061,1272.79 742.093,1274.56 750.125,1276.28 \n",
       "  758.157,1277.97 766.189,1279.61 774.221,1281.21 782.253,1282.77 790.285,1284.29 798.317,1285.78 806.349,1287.23 814.381,1288.65 822.413,1290.05 830.445,1291.07 \n",
       "  838.477,1292.41 846.509,1293.72 854.541,1295.01 862.573,1296.27 870.605,1297.51 878.637,1298.73 886.67,1299.92 894.702,1301.09 902.734,1302.25 910.766,1303.1 \n",
       "  918.798,1304.22 926.83,1305.32 934.862,1306.4 942.894,1307.47 950.926,1308.52 958.958,1309.56 966.99,1310.58 975.022,1311.58 983.054,1312.57 991.086,1313.55 \n",
       "  999.118,1314.51 1007.15,1315.46 1015.18,1316.39 1023.21,1317.31 1031.25,1318.22 1039.28,1319.12 1047.31,1320.01 1055.34,1320.89 1063.37,1321.76 1071.41,1322.61 \n",
       "  1079.44,1323.46 1087.47,1324.3 1095.5,1325.12 1103.53,1325.93 1111.57,1326.74 1119.6,1327.53 1127.63,1328.31 1135.66,1329.09 1143.69,1329.85 1151.73,1330.6 \n",
       "  1159.76,1331.35 1167.79,1332.09 1175.82,1332.81 1183.85,1333.54 1191.89,1334.25 1199.92,1334.78 1207.95,1335.48 1215.98,1336.18 1224.01,1336.86 1232.05,1337.54 \n",
       "  1240.08,1338.21 1248.11,1338.87 1256.14,1339.53 1264.17,1340.18 1272.21,1340.82 1280.24,1341.46 1288.27,1342.09 1296.3,1342.71 1304.34,1343.33 1312.37,1343.94 \n",
       "  1320.4,1344.54 1328.43,1345.14 1336.46,1345.74 1344.5,1346.33 1352.53,1346.91 1360.56,1347.49 1368.59,1348.06 1376.62,1348.63 1384.66,1349.19 1392.69,1349.74 \n",
       "  1400.72,1350.29 1408.75,1350.84 1416.78,1351.38 1424.82,1351.91 1432.85,1352.44 1440.88,1352.96 1448.91,1353.48 1456.94,1354 1464.98,1354.5 1473.01,1355.01 \n",
       "  1481.04,1355.5 1489.07,1356 1497.1,1356.48 1505.14,1356.97 1513.17,1357.45 1521.2,1357.92 1529.23,1358.39 1537.26,1358.86 1545.3,1359.32 1553.33,1359.78 \n",
       "  1561.36,1360.23 1569.39,1360.68 1577.42,1361.12 1585.46,1361.55 1593.49,1361.99 1601.52,1362.41 1609.55,1362.84 1617.58,1363.26 1625.62,1363.67 1633.65,1364.08 \n",
       "  1641.68,1364.49 1649.71,1364.89 1657.74,1365.29 1665.78,1365.68 1673.81,1366.07 1681.84,1366.46 1689.87,1366.84 1697.9,1367.22 1705.94,1367.6 1713.97,1367.97 \n",
       "  1722,1368.34 1730.03,1368.7 1738.06,1369.06 1746.1,1369.42 1754.13,1369.77 1762.16,1370.13 1770.19,1370.47 1778.22,1370.82 1786.26,1371.08 1794.29,1371.41 \n",
       "  1802.32,1371.75 1810.35,1372.08 1818.38,1372.41 1826.42,1372.74 1834.45,1373.06 1842.48,1373.38 1850.51,1373.7 1858.55,1374.01 1866.58,1374.32 1874.61,1374.63 \n",
       "  1882.64,1374.93 1890.67,1375.23 1898.71,1375.52 1906.74,1375.81 1914.77,1376.1 1922.8,1376.39 1930.83,1376.67 1938.87,1376.95 1946.9,1377.22 1954.93,1377.49 \n",
       "  1962.96,1377.76 1970.99,1378.03 1979.03,1378.29 1987.06,1378.55 1995.09,1378.81 2003.12,1379.07 2011.15,1379.32 2019.19,1379.57 2027.22,1379.81 2035.25,1380.06 \n",
       "  2043.28,1380.3 2051.31,1380.54 2059.35,1380.77 2067.38,1381 2075.41,1381.23 2083.44,1381.46 2091.47,1381.69 2099.51,1381.91 2107.54,1382.13 2115.57,1382.35 \n",
       "  2123.6,1382.57 2131.63,1382.78 2139.67,1382.99 2147.7,1383.2 2155.73,1383.41 2163.76,1383.62 2171.79,1383.82 2179.83,1384.02 2187.86,1384.22 2195.89,1384.41 \n",
       "  2203.92,1384.61 2211.95,1384.8 2219.99,1384.99 2228.02,1385.18 2236.05,1385.36 2244.08,1385.55 2252.11,1385.73 2260.15,1385.91 2268.18,1386.09 2276.21,1386.26 \n",
       "  2284.24,1386.44 2292.27,1386.61 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0502)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  276.235,90.3488 284.267,174.37 292.299,288.475 300.331,397.741 308.363,494.49 316.395,578.632 324.427,651.134 332.459,713.437 340.492,767.437 348.524,814.688 \n",
       "  356.556,856.152 364.588,892.656 372.62,924.861 380.652,953.142 388.684,977.933 396.716,999.725 404.748,1018.96 412.78,1035.91 420.812,1050.88 428.844,1064.2 \n",
       "  436.876,1076.06 444.908,1086.65 452.94,1096.17 460.972,1104.78 469.004,1112.6 477.036,1119.71 485.068,1126.2 493.1,1132.13 501.132,1137.56 509.164,1142.59 \n",
       "  517.196,1147.25 525.228,1151.56 533.26,1155.56 541.292,1159.28 549.324,1162.74 557.356,1165.97 565.388,1169.01 573.42,1171.86 581.452,1174.52 589.484,1177.01 \n",
       "  597.516,1179.35 605.548,1181.55 613.581,1183.61 621.613,1185.55 629.645,1187.39 637.677,1189.13 645.709,1190.77 653.741,1192.32 661.773,1193.79 669.805,1195.18 \n",
       "  677.837,1196.5 685.869,1197.76 693.901,1198.95 701.933,1200.07 709.965,1201.14 717.997,1202.14 726.029,1203.12 734.061,1204.06 742.093,1204.94 750.125,1205.78 \n",
       "  758.157,1206.6 766.189,1207.37 774.221,1208.11 782.253,1208.82 790.285,1209.48 798.317,1210.11 806.349,1210.71 814.381,1211.29 822.413,1211.83 830.445,1212.21 \n",
       "  838.477,1212.69 846.509,1213.15 854.541,1213.59 862.573,1214.01 870.605,1214.41 878.637,1214.78 886.67,1215.14 894.702,1215.47 902.734,1215.77 910.766,1216 \n",
       "  918.798,1216.3 926.83,1216.56 934.862,1216.81 942.894,1217.07 950.926,1217.3 958.958,1217.5 966.99,1217.69 975.022,1217.88 983.054,1218.05 991.086,1218.2 \n",
       "  999.118,1218.34 1007.15,1218.48 1015.18,1218.6 1023.21,1218.72 1031.25,1218.81 1039.28,1218.9 1047.31,1218.99 1055.34,1219.07 1063.37,1219.14 1071.41,1219.19 \n",
       "  1079.44,1219.22 1087.47,1219.26 1095.5,1219.3 1103.53,1219.32 1111.57,1219.33 1119.6,1219.33 1127.63,1219.32 1135.66,1219.28 1143.69,1219.25 1151.73,1219.22 \n",
       "  1159.76,1219.18 1167.79,1219.14 1175.82,1219.09 1183.85,1219.03 1191.89,1218.98 1199.92,1218.94 1207.95,1218.87 1215.98,1218.81 1224.01,1218.75 1232.05,1218.66 \n",
       "  1240.08,1218.58 1248.11,1218.5 1256.14,1218.41 1264.17,1218.31 1272.21,1218.22 1280.24,1218.13 1288.27,1218.03 1296.3,1217.93 1304.34,1217.84 1312.37,1217.73 \n",
       "  1320.4,1217.61 1328.43,1217.49 1336.46,1217.35 1344.5,1217.21 1352.53,1217.07 1360.56,1216.93 1368.59,1216.78 1376.62,1216.63 1384.66,1216.48 1392.69,1216.32 \n",
       "  1400.72,1216.17 1408.75,1216.01 1416.78,1215.87 1424.82,1215.71 1432.85,1215.55 1440.88,1215.37 1448.91,1215.2 1456.94,1215.01 1464.98,1214.83 1473.01,1214.65 \n",
       "  1481.04,1214.47 1489.07,1214.28 1497.1,1214.1 1505.14,1213.91 1513.17,1213.71 1521.2,1213.5 1529.23,1213.29 1537.26,1213.08 1545.3,1212.86 1553.33,1212.65 \n",
       "  1561.36,1212.44 1569.39,1212.23 1577.42,1212.02 1585.46,1211.8 1593.49,1211.56 1601.52,1211.34 1609.55,1211.1 1617.58,1210.88 1625.62,1210.64 1633.65,1210.4 \n",
       "  1641.68,1210.16 1649.71,1209.91 1657.74,1209.67 1665.78,1209.42 1673.81,1209.17 1681.84,1208.91 1689.87,1208.68 1697.9,1208.43 1705.94,1208.17 1713.97,1207.94 \n",
       "  1722,1207.7 1730.03,1207.45 1738.06,1207.17 1746.1,1206.91 1754.13,1206.68 1762.16,1206.41 1770.19,1206.13 1778.22,1205.88 1786.26,1205.69 1794.29,1205.43 \n",
       "  1802.32,1205.16 1810.35,1204.85 1818.38,1204.58 1826.42,1204.3 1834.45,1204 1842.48,1203.73 1850.51,1203.44 1858.55,1203.16 1866.58,1202.88 1874.61,1202.61 \n",
       "  1882.64,1202.33 1890.67,1202.04 1898.71,1201.76 1906.74,1201.49 1914.77,1201.21 1922.8,1200.91 1930.83,1200.63 1938.87,1200.35 1946.9,1200.05 1954.93,1199.77 \n",
       "  1962.96,1199.47 1970.99,1199.18 1979.03,1198.88 1987.06,1198.58 1995.09,1198.3 2003.12,1198.01 2011.15,1197.72 2019.19,1197.41 2027.22,1197.11 2035.25,1196.8 \n",
       "  2043.28,1196.48 2051.31,1196.17 2059.35,1195.86 2067.38,1195.55 2075.41,1195.23 2083.44,1194.92 2091.47,1194.61 2099.51,1194.29 2107.54,1193.97 2115.57,1193.66 \n",
       "  2123.6,1193.33 2131.63,1193.01 2139.67,1192.68 2147.7,1192.37 2155.73,1192.05 2163.76,1191.72 2171.79,1191.39 2179.83,1191.07 2187.86,1190.75 2195.89,1190.42 \n",
       "  2203.92,1190.1 2211.95,1189.77 2219.99,1189.43 2228.02,1189.12 2236.05,1188.78 2244.08,1188.45 2252.11,1188.13 2260.15,1187.8 2268.18,1187.48 2276.21,1187.16 \n",
       "  2284.24,1186.84 2292.27,1186.51 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip0500)\" d=\"\n",
       "M1947.14 312.204 L2280.76 312.204 L2280.76 130.764 L1947.14 130.764  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1947.14,312.204 2280.76,312.204 2280.76,130.764 1947.14,130.764 1947.14,312.204 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1971.14,191.244 2115.14,191.244 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2139.14, 208.744)\" x=\"2139.14\" y=\"208.744\">train</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip0500)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1971.14,251.724 2115.14,251.724 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip0500)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2139.14, 269.224)\" x=\"2139.14\" y=\"269.224\">test</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = Float64[]\n",
    "test_loss = Float64[]\n",
    "Flux.train!(L, par, Iterators.repeated(trainbatch, 1000), opt;\n",
    "            cb = Flux.throttle(update_loss!, 1))\n",
    "plot(1:length(train_loss), train_loss, xlabel=\"~seconds of training\", ylabel=\"loss\", label=\"train\")\n",
    "plot!(1:length(test_loss), test_loss, label=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about image structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final note, notice that our model doesn't take into account any aspect of the image's connected-ness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAALPSURBVGje7dpNqE1RFAfw3/FeSYRMfPYMpKd8lAzIR4koBuLF5JWBvAwwR4ryEUoGBoqRUiY8lFCUlFJvIOVjIL2Jokx8RS8fF4N9bvfc45x7ewP3Pued/+Ts1l57r7v2umvttdfe0UdMlo/pWIbrqKCjAe8+nGrQX8EYLUbxBZb4/xGlCa/RheM4GNPG42uCp4IhTMBVbMVSDKj5atJnk+3iu0XLBUabcSNBmIL36tf9IVYleCrxNyuuVsddwbaY9gpzMa4dGhZfYGe/sOarcVSwVQX9+IwPmBUzV+2zAysTk1R9sYLumLYt0b82/n5ph4bFFxjl5SmDmINNQj6TjI/T0IszqTEDQkzNQ5nT/BNE63E3o+M+1sTtLDun98gRq2HxBeb6YVZOskXwyReYlzFujBCT76foU/EOj9qhYfEFRovwNONX/IrbSVvewYaMSZ5gsebnxyPt0LD4AjtvYnaK+EvYCwcFm3zCJNzKmWRxdbImwrraoWHxBZb4/xE1Y0jGx+24lMFzD+vwU2NfnGw0uEXrY2makN7Tku2q/Y7hND4m+E/Gk53AgdQ8z7Eg5i/+krY+p8nrSNqgF5dTg85jV8a4iUJtYMRoWHyBnckcNA996m34W8hzsvC5yVzFX9IypykxbETVGvdh7Bfq0peF/OWnUA/fGfMcEs54aczAW2FP3N9A2BSjwS1avx++wcyMjm68xHdcwF75Z/jb2Kj5Gb+sl5YokYnMvHQ1HsTtZr4F3zA28W2E4rtF62Np3v1D0naPsQTzhXr3Hpz1t20vxBr0peib1e6Zi7+kZSwtMWzU3T1V85ge4S3FCuENxUX1d8Dptxg9uIYfwr+wQ6iDP4n73ws5aZnTlChRoqCIHmF5RkfWPf6QUAPIe9f2SXiv2KH2Po7au7bbRkNoK2NpiWEjWohnDRjOYXfczjsrzsQbzc+Sd40Gt2i5wD+UTov9gEaUkwAAAABJRU5ErkJggg==",
      "text/plain": [
       "28×28 Array{Gray{N0f8},2} with eltype Gray{Normed{UInt8,8}}:\n",
       " Gray{N0f8}(0.941)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.992)\n",
       " Gray{N0f8}(0.0)    Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  \n",
       " Gray{N0f8}(0.0)    Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  \n",
       " Gray{N0f8}(0.314)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.035)\n",
       " Gray{N0f8}(0.992)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.992)\n",
       " Gray{N0f8}(0.992)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.992)\n",
       " Gray{N0f8}(0.275)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  \n",
       " Gray{N0f8}(0.992)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.776)\n",
       " Gray{N0f8}(0.18)   Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.51) \n",
       " Gray{N0f8}(0.882)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.627)\n",
       " Gray{N0f8}(0.0)    Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  \n",
       " Gray{N0f8}(0.0)    Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.063)\n",
       " Gray{N0f8}(0.0)    Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  \n",
       " ⋮                                   ⋱                                    \n",
       " Gray{N0f8}(0.071)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.071)\n",
       " Gray{N0f8}(0.898)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.992)\n",
       " Gray{N0f8}(0.0)    Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  \n",
       " Gray{N0f8}(0.0)    Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  \n",
       " Gray{N0f8}(0.043)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  \n",
       " Gray{N0f8}(0.0)    Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  \n",
       " Gray{N0f8}(0.992)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.992)\n",
       " Gray{N0f8}(0.0)    Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  \n",
       " Gray{N0f8}(0.0)    Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  \n",
       " Gray{N0f8}(0.776)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.714)\n",
       " Gray{N0f8}(0.0)    Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  \n",
       " Gray{N0f8}(0.176)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.729)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random\n",
    "p = randperm(28)\n",
    "images[1][p,p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.215686, 0.533333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67451, 0.992157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0705882, 0.886275, 0.992157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.192157, 0.0705882, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.670588, 0.992157, 0.992157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.117647, 0.933333, 0.858824, 0.313725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0901961, 0.858824, 0.992157, 0.831373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.141176, 0.992157, 0.992157, 0.611765, 0.054902, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.258824, 0.992157, 0.992157, 0.529412, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.368627, 0.992157, 0.992157, 0.419608, 0.00392157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0941176, 0.835294, 0.992157, 0.992157, 0.517647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.603922, 0.992157, 0.992157, 0.992157, 0.603922, 0.545098, 0.0431373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.447059, 0.992157, 0.992157, 0.956863, 0.0627451, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0117647, 0.666667, 0.992157, 0.992157, 0.992157, 0.992157, 0.992157, 0.745098, 0.137255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.152941, 0.866667, 0.992157, 0.992157, 0.521569, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0705882, 0.992157, 0.992157, 0.992157, 0.803922, 0.352941, 0.745098, 0.992157, 0.945098, 0.317647, 0.0, 0.0, 0.0, 0.0, 0.580392, 0.992157, 0.992157, 0.764706, 0.0431373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0705882, 0.992157, 0.992157, 0.776471, 0.0431373, 0.0, 0.00784314, 0.27451, 0.882353, 0.941176, 0.176471, 0.0, 0.0, 0.180392, 0.898039, 0.992157, 0.992157, 0.313725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0705882, 0.992157, 0.992157, 0.713725, 0.0, 0.0, 0.0, 0.0, 0.627451, 0.992157, 0.729412, 0.0627451, 0.0, 0.509804, 0.992157, 0.992157, 0.776471, 0.0352941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.494118, 0.992157, 0.992157, 0.968627, 0.168627, 0.0, 0.0, 0.0, 0.423529, 0.992157, 0.992157, 0.364706, 0.0, 0.717647, 0.992157, 0.992157, 0.317647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.533333, 0.992157, 0.984314, 0.945098, 0.603922, 0.0, 0.0, 0.0, 0.00392157, 0.466667, 0.992157, 0.988235, 0.976471, 0.992157, 0.992157, 0.788235, 0.00784314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.686275, 0.882353, 0.364706, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0980392, 0.588235, 0.992157, 0.992157, 0.992157, 0.980392, 0.305882, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.101961, 0.67451, 0.321569, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.105882, 0.733333, 0.976471, 0.811765, 0.713725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65098, 0.992157, 0.321569, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25098, 0.00784314, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.94902, 0.219608, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.968627, 0.764706, 0.152941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.498039, 0.25098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
     ]
    }
   ],
   "source": [
    "show(preprocess(images[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22m \u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22m \u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22m! \u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22mert \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22mDims \u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22mexhull \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22mTranspose ∇\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22m_data\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "Conv(size, in=>out)\n",
       "Conv(size, in=>out, relu)\n",
       "\\end{verbatim}\n",
       "Standard convolutional layer. \\texttt{size} should be a tuple like \\texttt{(2, 2)}. \\texttt{in} and \\texttt{out} specify the number of input and output channels respectively.\n",
       "\n",
       "Example: Applying Conv layer to a 1-channel input using a 2x2 window size,          giving us a 16-channel output. Output is activated with ReLU.\n",
       "\n",
       "\\begin{verbatim}\n",
       "size = (2,2)\n",
       "in = 1\n",
       "out = 16\n",
       "Conv((2, 2), 1=>16, relu)\n",
       "\\end{verbatim}\n",
       "Data should be stored in WHCN order (width, height, \\# channels, \\# batches). In other words, a 100×100 RGB image would be a \\texttt{100×100×3×1} array, and a batch of 50 would be a \\texttt{100×100×3×50} array.\n",
       "\n",
       "Takes the keyword arguments \\texttt{pad}, \\texttt{stride} and \\texttt{dilation}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "Conv(size, in=>out)\n",
       "Conv(size, in=>out, relu)\n",
       "```\n",
       "\n",
       "Standard convolutional layer. `size` should be a tuple like `(2, 2)`. `in` and `out` specify the number of input and output channels respectively.\n",
       "\n",
       "Example: Applying Conv layer to a 1-channel input using a 2x2 window size,          giving us a 16-channel output. Output is activated with ReLU.\n",
       "\n",
       "```\n",
       "size = (2,2)\n",
       "in = 1\n",
       "out = 16\n",
       "Conv((2, 2), 1=>16, relu)\n",
       "```\n",
       "\n",
       "Data should be stored in WHCN order (width, height, # channels, # batches). In other words, a 100×100 RGB image would be a `100×100×3×1` array, and a batch of 50 would be a `100×100×3×50` array.\n",
       "\n",
       "Takes the keyword arguments `pad`, `stride` and `dilation`.\n"
      ],
      "text/plain": [
       "\u001b[36m  Conv(size, in=>out)\u001b[39m\n",
       "\u001b[36m  Conv(size, in=>out, relu)\u001b[39m\n",
       "\n",
       "  Standard convolutional layer. \u001b[36msize\u001b[39m should be a tuple like \u001b[36m(2, 2)\u001b[39m. \u001b[36min\u001b[39m and \u001b[36mout\u001b[39m\n",
       "  specify the number of input and output channels respectively.\n",
       "\n",
       "  Example: Applying Conv layer to a 1-channel input using a 2x2 window size,\n",
       "  giving us a 16-channel output. Output is activated with ReLU.\n",
       "\n",
       "\u001b[36m  size = (2,2)\u001b[39m\n",
       "\u001b[36m  in = 1\u001b[39m\n",
       "\u001b[36m  out = 16\u001b[39m\n",
       "\u001b[36m  Conv((2, 2), 1=>16, relu)\u001b[39m\n",
       "\n",
       "  Data should be stored in WHCN order (width, height, # channels, # batches).\n",
       "  In other words, a 100×100 RGB image would be a \u001b[36m100×100×3×1\u001b[39m array, and a\n",
       "  batch of 50 would be a \u001b[36m100×100×3×50\u001b[39m array.\n",
       "\n",
       "  Takes the keyword arguments \u001b[36mpad\u001b[39m, \u001b[36mstride\u001b[39m and \u001b[36mdilation\u001b[39m."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Conv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.5",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
